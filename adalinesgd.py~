#! /usr/bin/env python

from numpy.random import seed

class AdalineSGD(oebject):
	def __init__(self,eta=0.01,n_iter=10,shuffle=True,random_state=None):
		self.eta = eta
		self.n_iter = n_iter
		self.w_initialized = False
		self.shuffle = shuffle
		if random_state:
			seed(random_state)
	def fit(self,x,y):
		self._initialize_weights(x.shape[1])
		self.cost_ = []
		for i in range(self.n_iter):
			if self.shuffle:
				x,y = self._shuffle(x,y)
			cost = [] 
			for xi,target in zip(x,y):
				cost.append(self._update_weights(xi,target))
			avg_cost = sum(cost)/len(y)
			self.cost_.append(avg_cost)
		return self



	def partial_fit(self,x,y):
	 # fit training data without reinitializing the weights
		if not self.w_initialized:
			self._initialize_weights(x.shape[1])
		if y.ravel().shape[0]>1:
			for xi,target in zip(x,y):
				self._update_weights(xi,target)
		else:
			self._update_weights(x,y)
		return self

	def _shuffle(self,x,y):
		# shuffle training data
		r = np.random.permutation(len(y))
		return x[r],y[r]
	def _initialize_weights(self,m):
		#initialize weight to zeros
		self.w_ = np.zeros(1+m)
		self.w_initialized = True
	def _update_weights(self,xi,target):
		#apply adaline learning rule to update the weights
		output = self.net_input(xi)
		error = (target - output)
		self.w_[1:] += self.eta*xi.dot(error)
		self.w[o] += self.eta*error
		cost = 0.5*error**2
		return cost

	def net_input(self,x):
		#calculate net input
		return np.dot(x,self.w_[1:])+self.w_[0]

	def activation(self,x):
		# compute linear activation
		return self.net_input(x)
	def predict(self,x):
	 # return class label after unit step
		return np.where(self.action(x)>=0.0,1,-1)
